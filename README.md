# House Prices Prediction

Этот проект представляет собой полный анализ и моделирование цен на дома из датасета Kaggle House Prices - Advanced Regression Techniques. Jupyter notebook выполняет EDA, предобработку данных и обучение моделей с использованием градиентного бустинга.

**Цель соревнования**: Предсказать итоговую цену продажи каждого дома по 79 признакам, описывающим различные аспекты жилых домов.

 **Метрика оценки**: Root-Mean-Squared-Error (RMSE) между логарифмом предсказанной цены и логарифмом наблюдаемой цены.
 
 $$ \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (\log(y_i) - \log(\hat{y}_i))^2} $$

 **Данные**:
 - `train.csv` - обучающая выборка с целевой переменной `SalePrice`
 - `test.csv` - тестовая выборка без целевой переменной
 - `data_description.txt` - описание всех признаков 
 - `sample_submission.csv` - пример файла для отправки
## Описание датасета

Датасет содержит информацию о 1460 домах (train.csv) с 79 признаками, включая характеристики участка, качество материалов, площадь и т.д. Целевая переменная — SalePrice.

Ключевые статистики:

- SalePrice: mean 180,921; median 163,000; std 79,442; min 34,900; max 755,000.
- Основные признаки: OverallQual (0.79 корреляция), GrLivArea (0.71), TotalBsmtSF (0.61).

Данные объединены из train (1460x81) и test (1459x80) для предобработки, с разделением индексов.

## Основные шаги анализа

### 1. Исследовательский анализ (EDA)

- Загрузка и просмотр данных (train.shape: 1460x81).
- Описание: 43 категориальных признака, много пропусков (Alley: 91/1460).
- Визуализация: гистограмма SalePrice (правый хвост), log-трансформация, QQ-plot.
- Корреляции с SalePrice.


### 2. Предобработка

- Обработка пропусков, категориальных переменных (target/ordinal/one-hot encoding).
- Log-трансформация SalePrice для нормализации.
- Создание новых признаков (Feature Engineering).

### 3. Моделирование

- Базовые модели: RandomForest, GradientBoosting.
- Продвинутые: XGBoost, LightGBM.

| Модель | Train RMSE | Val RMSE |
| :-- | :-- | :-- |
| RandomForest | 0.047 | 0.1417 |
| GradientBoost | 0.047 | 0.1387 |
| XGBoost | 0.046 | 0.1312 |
| LightGBM | 0.042 | 0.1355 |

- На основе результатов была выбрана модель XGBoost, показавшая лучший результат.
- Для этой модели были отобраны наиболее важные признаки по принципу Cumulative 95%
- И также проведена гиперпараметрная оптимизация с Optuna (100 trials).
    - Лучшие параметры XGBoost: n_estimators=999, learning_rate=0.024, max_depth=3 и др.
    - CV RMSE (log): ~0.1184 (trial 96).


## Результаты

- Лучшая модель: XGBoost с CV RMSE 0.1184 на log(SalePrice) на train.csv.
- Предсказания для test.csv 0.12267 (топ 10%) на сайте Kaggle.

## Установка и запуск

1. Клонируйте репозиторий.
2. Установите зависимости:

```
pip install numpy pandas matplotlib seaborn scikit-learn xgboost lightgbm optuna
```

3. Скачайте данные Kaggle: train.csv, test.csv.
4. Запустите `House_Prices.ipynb` в Jupyter.

Проект готов к улучшениям: stacking моделей, больше фичей.


